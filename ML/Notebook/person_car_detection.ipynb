{"cells":[{"cell_type":"markdown","source":["#INTRODUCTION\n","This notebook introduces training and testing of a Convolution Neural Network (CNN) model using keras and Tensorflow. The model in question will later be used in a device running esp32 to guide a blind person by identifying and clarifying cars and people this avoiding possible collision.\n","As the model will be deployed in a resource constrained device, the notebook goes into detail on how edge impulse will be used to deploy the Tensorflow lite model which has been obtained from conversion of the original model to a tflite model."],"metadata":{"id":"8VlcNiCGkM_b"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6jKMr-vT3jk"},"outputs":[],"source":["import numpy as np\n","import os\n","import matplotlib.pyplot as plt\n","import cv2\n","from google.colab import drive\n","import time\n","from datetime import datetime\n","\n","t = datetime.now().strftime(\"%H:%M\")\n","model_name = 'gray'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvSdtSlJVTc8"},"outputs":[],"source":["# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Create a folder in Google Drive\n","data_path = '/content/drive/MyDrive/dataset'\n","os.makedirs(data_path, exist_ok=True)\n","print(f\"Folder '{data_path}' created in Google Drive.\")\n","\n","save_directory = '/content/drive/MyDrive/models'\n","os.makedirs(save_directory, exist_ok=True)\n","save_path = os.path.join (save_directory, f\"{model_name}.keras\")\n","\n","\n","save_directory = '/content/drive/MyDrive/models'\n","os.makedirs(save_directory, exist_ok=True)\n","save_path_lite = os.path.join (save_directory, f\"{model_name}.tflite\")\n","\n","\n","Test_dir = '/content/drive/MyDrive/Test_img'\n","os.makedirs(Test_dir, exist_ok=True)\n","Test_folder= '/content/drive/MyDrive/Test_img/test'\n","os.makedirs(Test_folder, exist_ok=True)\n","\n","Ard_dir = '/content/drive/MyDrive/Arduino'\n","os.makedirs(Ard_dir, exist_ok=True)\n","Lib_folder= '/content/drive/MyDrive/Arduino/Library'\n","os.makedirs(Lib_folder, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O8GSLIznWQdA"},"outputs":[],"source":["DATADIR = data_path\n","CATEGORIES = ['person','car']\n","\n","for category in CATEGORIES:\n","    path = os.path.join (DATADIR, category)\n","    for img in os.listdir (path):\n","        img_array = cv2.imread (os.path.join(path, img),cv2.IMREAD_GRAYSCALE)\n","        img_array = img_array/255\n","        plt.imshow (img_array, cmap = 'gray')\n","        plt.show ()\n","        break\n","    break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2VLMWPSDexyA"},"outputs":[],"source":["IMG_SIZE = 50\n","new_array = cv2.resize (img_array, (IMG_SIZE, IMG_SIZE))\n","plt.imshow (new_array, cmap = 'gray')\n","plt.show ()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p265uaOph-3Q"},"outputs":[],"source":["training_data = []\n","\n","def create_training_data ():\n","    for category in CATEGORIES:\n","        path = os.path.join (DATADIR, category)\n","        class_num = CATEGORIES.index (category)\n","        for img in os.listdir (path):\n","            try:\n","                img_array = cv2.imread (os.path.join(path, img),cv2.IMREAD_GRAYSCALE)\n","                new_array = cv2.resize (img_array, (IMG_SIZE, IMG_SIZE))\n","                new_array = new_array/255\n","                training_data.append ([new_array, class_num])\n","            except Exception as e:\n","                pass\n","\n","create_training_data ()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gm39W-prt4qO"},"outputs":[],"source":["import random\n","for i in range (5678):\n","    random.shuffle (training_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwMMH5AAv2nF"},"outputs":[],"source":["from tensorflow.keras.utils import to_categorical\n","X = []\n","y = []\n","for features, label in training_data:\n","    X.append (features)\n","    y.append (label)\n","X = np.array (X).reshape (-1,IMG_SIZE,IMG_SIZE,1)\n","y = np.array (y)\n","count_zeros = np.count_nonzero(y == 1)\n","print (count_zeros)\n","y = to_categorical(y, num_classes=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2ibG-IWOBzT"},"outputs":[],"source":["print (y.shape)\n","print (X.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"VMG_8K3TrYdK"},"outputs":[],"source":["print (len(training_data))\n","for sample in training_data[:10]:\n","   print (sample[1] )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7sWb7JavxfMx"},"outputs":[],"source":["#saving training data\n","import pickle\n","save_directory ='/content/drive/MyDrive/Training_data'\n","os.makedirs(save_directory, exist_ok=True)\n","save_path_X = os.path.join (save_directory, f\"X-{model_name}.pickle\")\n","save_path_y = os.path.join (save_directory, f\"y-{model_name}.pickle\")\n","\n","pickle_out = open (save_path_X, \"wb\")\n","pickle.dump (X, pickle_out)\n","pickle_out.close ()\n","\n","pickle_out = open (save_path_y, \"wb\")\n","pickle.dump (y, pickle_out)\n","pickle_out.close ()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6zQ4Y1VDyW7L"},"outputs":[],"source":["\n","#loading training data\n","import pickle\n","save_directory ='/content/drive/MyDrive/Training_data'\n","os.makedirs(save_directory, exist_ok=True)\n","save_path_X = os.path.join (save_directory, f\"X-{model_name}.pickle\")\n","save_path_y = os.path.join (save_directory, f\"y-{model_name}.pickle\")\n","\n","\n","pickle_in = open (save_path_X, \"rb\")\n","X = pickle.load (pickle_in)\n","\n","pickle_in = open (save_path_y, \"rb\")\n","y = pickle.load (pickle_in)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yczFPRMFFU9L"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","datagen = ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.15,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.3,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")"]},{"cell_type":"markdown","metadata":{"id":"_yvT127R3psh"},"source":["#Convolution Neural Network\n","Tensorflow and keras are used to train the model. The various parameters are adjusted until the model achieves an accuracy that can be used in real world scenario."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cTuUTL4xINh"},"outputs":[],"source":["\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, Activation\n","from tensorflow.keras import regularizers\n","from sklearn.model_selection import train_test_split\n","\n","l_r = 0.0001\n","Epochs = 15\n","opt = Adam(learning_rate = l_r)\n","batch_size = 32\n","\n","X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n","model = Sequential()\n","\n","model.add(Conv2D(64, (3, 3), input_shape=X.shape[1:],kernel_regularizer=regularizers.l1(0.01)))\n","model.add(Activation(\"relu\"))\n","model.add(MaxPooling2D(pool_size=(2, 2)))\n","model.add(Dropout(0.2))\n","\n","model.add(Conv2D(64, (3,3)))\n","model.add(Activation(\"relu\"))\n","model.add(MaxPooling2D(pool_size = (2,2)))\n","model.add(Dropout(0.1))\n","\n","model.add(Flatten())\n","model.add(Dense(32))\n","model.add(Activation(\"relu\"))\n","\n","model.add(Dense(2))\n","model.add(Activation (\"softmax\"))\n","\n","\n","model.compile(loss = \"binary_crossentropy\",\n","              optimizer = opt,\n","              metrics = [\"accuracy\"])\n","\n","datagen.fit(X_train)\n","\n","train_generator = datagen.flow(X_train, y_train, batch_size = batch_size)\n","\n","#print(type(train_generator))\n","\n","history = model.fit(train_generator, steps_per_epoch=len(X_train) // batch_size, epochs=Epochs, validation_data=(X_val, y_val))\n","\n","#checkpoint = ModelCheckpoint(save_path,save_freq = 'epoch', monitor='val_loss', save_best_only=True, save_weights_only=False, mode='auto')\n","#history = model.fit(X,y, batch_size = 32, epochs = Epochs, verbose = 1, callbacks = [checkpoint])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpCfyYdlCcPe"},"outputs":[],"source":["model.summary ()"]},{"cell_type":"code","source":["model.save(save_path)"],"metadata":{"id":"vF6IXfZru_nE"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jmy2dKAj58f6"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['accuracy'], label='accuracy')\n","plt.plot(history.history['val_accuracy'], label='val_accuracy')\n","plt.plot(history.history['loss'], label='loss')\n","plt.plot(history.history['val_loss'], label='val_loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Metric Value')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","source":["Image test"],"metadata":{"id":"47Uwb11Qv3QK"}},{"cell_type":"code","source":["import tensorflow as tf\n","model = tf.keras.models.load_model(save_path)\n","\n","print(f\"Model loaded successfully: {model}\")"],"metadata":{"id":"X0dm0GlEv0yP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SoD9byjfXHzS"},"outputs":[],"source":["directory_path = Test_dir\n","folder_name = Test_folder\n","test_folder_path = os.path.join(directory_path, folder_name)\n","IMG_SIZE = 50\n","\n","if os.path.exists(test_folder_path):\n","    image_files = [f for f in os.listdir(test_folder_path) if os.path.isfile(os.path.join(test_folder_path, f))]\n","    for image_file in image_files:\n","        image_path = os.path.join(test_folder_path, image_file)\n","        img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n","        plt.imshow (img, cmap = 'gray' )\n","        plt.show ()\n","        img = cv2.resize(img,(IMG_SIZE,IMG_SIZE))\n","        img = img/255\n","        input_img = np.expand_dims(img, axis=0)\n","        Predictions = model.predict(input_img)\n","\n","        predicted_class = CATEGORIES[np.argmax(Predictions)]\n","        confidence = np.max(Predictions)\n","\n","\n","        if  confidence >= 0.8:\n","            print (predicted_class, confidence)\n","        else:\n","            print (\"unrecognisable class\")\n","else:\n","    print(f\"The folder '{folder_name}' doesn't exist\")"]},{"cell_type":"markdown","metadata":{"id":"UWbqtmQF_WCD"},"source":["#Converting to tflite\n","Tensorflow lite allows models to run on resource constrained devices such raspberry pi and other microcontrollers. The conversion of a keras model is document on [Tensorflow's docs](https://www.tensorflow.org/api_docs/python/tf/lite/TFLiteConverter)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cRFhH6lc9glE"},"outputs":[],"source":["import tensorflow as tf\n","\n","DATADIR = data_path\n","CATEGORIES = ['person', 'car']\n","IMG_SIZE = 50\n","\n","def representative_data_gen():\n","    data_dir = DATADIR\n","    batch_size = 32\n","    img_height = IMG_SIZE\n","    img_width = IMG_SIZE\n","\n","    class_names = CATEGORIES\n","\n","    train_images = tf.keras.preprocessing.image_dataset_from_directory(\n","        data_dir,\n","        class_names=class_names,\n","        validation_split=0.2,\n","        subset=\"training\",\n","        seed=123,\n","        image_size=(img_height, img_width),\n","        batch_size=batch_size)\n","\n","    #standardize the images\n","    normalization_layer = tf.keras.layers.experimental.preprocessing.Rescaling(1/255)\n","    normalized_ds = train_images.map(lambda x, y: (normalization_layer(x), y))\n","    image_batch, labels_batch = next((iter(normalized_ds)))\n","    first_image = image_batch[0]\n","    #print(image_batch)\n","\n","    print(np.min(first_image), np.max(first_image))\n","\n","    for input_value in tf.data.Dataset.from_tensor_slices(image_batch).batch(1).take(100):\n","        print(input_value.shape)\n","        # Model has only one input so each data point has one element.\n","        yield [tf.constant(input_value, dtype=tf.float32, shape=(1, img_height, img_width, 1))]"]},{"cell_type":"code","source":["representative_data_gen()"],"metadata":{"id":"EuUTpU9Q6ox2"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xhaHONOSij-4"},"outputs":[],"source":["converter = tf.lite.TFLiteConverter.from_keras_model(model)\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","converter.representative_dataset = representative_data_gen\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.uint8\n","converter.inference_output_type = tf.uint8\n","tflite_quant_model = converter.convert()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HZoGtDbvilaC"},"outputs":[],"source":["with open(save_path_lite, \"wb\") as f:\n","    f.write(tflite_quant_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9UdKms0YirBr"},"outputs":[],"source":["interpreter = tf.lite.Interpreter(save_path_lite)\n","interpreter.allocate_tensors()\n","\n","input_details = interpreter.get_input_details()\n","output_details = interpreter.get_output_details()\n","\n","print(\"Input details:\", input_details)\n","print(\"Output details:\", output_details)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAn0BQ74ixOt"},"outputs":[],"source":["summary = interpreter.get_signature_list()\n","print(\"Model Summary:\", summary)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UD-tEIsTi159"},"outputs":[],"source":["input_tensor_shape = input_details[0]['shape']\n","output_tensor_shape = output_details[0]['shape']\n","input_tensor_type = input_details[0]['dtype']\n","output_tensor_type = output_details[0]['dtype']\n","\n","print(\"Input Tensor Shape:\", input_tensor_shape)\n","print(\"Output Tensor Shape:\", output_tensor_shape)\n","print(\"Input Tensor Type:\", input_tensor_type)\n","print(\"Output Tensor Type:\", output_tensor_type)"]},{"cell_type":"markdown","metadata":{"id":"5aD6VjuF_SFe"},"source":["#Model deployment\n","As deployment will be on resource constrained devices, such as esp32, [edge impulse](https://docs.edgeimpulse.com/docs/edge-impulse-studio/deployment) will be used to deploy the model to an Arduino-compatible library that will run on the microcontroller."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZt1MSou9h2D"},"outputs":[],"source":["!pip install edgeimpulse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DWb27dmw9lYV"},"outputs":[],"source":["import edgeimpulse as ei\n","from google.colab import userdata\n","\n","API_KEY = userdata.get('EI_API_KEY')\n","\n","ei.API_KEY = API_KEY"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bat0UQdG9iP4"},"outputs":[],"source":["ei.model.list_profile_devices()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4M3TxOn89mMw"},"outputs":[],"source":["profile = ei.model.profile(model=tflite_model, device='espressif-esp32')\n","print(profile.summary())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHJEHB38DrXQ"},"outputs":[],"source":["print(f\"Estimated RAM usage: {profile.model.profile_info.float32.memory.tflite.ram}\")\n","print(f\"Estimated ROM usage: {profile.model.profile_info.float32.memory.tflite.rom}\")\n","print(f\"Estimated inference time (ms): {profile.model.profile_info.float32.time_per_inference_ms}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GCDV0jOL9ktP"},"outputs":[],"source":["ei.model.list_deployment_targets()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTBsBqFL9j2f"},"outputs":[],"source":["ei.model.deploy(model=tflite_model,\n","                model_input_type=ei.model.input_type.OtherInput(),\n","                model_output_type=ei.model.output_type.Classification(),\n","                output_directory=Lib_folder)"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNdQ6MQ04tCXQ6lyRVU1eGd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}